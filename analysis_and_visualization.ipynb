{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Visualization of Benchmark Results\n",
    "\n",
    "This notebook loads the results generated by `run_benchmark.py` and reproduces the key figures and tables from the paper *'Reproducible Benchmark of Wavelet-Enhanced Intrabody Communication Biometric Identification'*.\n",
    "\n",
    "The primary goal is to provide a clear and reproducible workflow for analyzing the benchmark outcomes without re-running the experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Loading Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot style for better aesthetics\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "RESULTS_DIR = 'results'\n",
    "\n",
    "print(f\"Loading results from: {RESULTS_DIR}/\")\n",
    "\n",
    "# Find all JSON result files\n",
    "result_files = glob.glob(os.path.join(RESULTS_DIR, '*.json'))\n",
    "\n",
    "# Load and parse results into a list of dictionaries\n",
    "all_results = []\n",
    "for f_path in result_files:\n",
    "    with open(f_path, 'r') as f:\n",
    "        res = json.load(f)\n",
    "        # Flatten the nested structure for easier DataFrame creation\n",
    "        flat_res = {\n",
    "            'model': res['config']['model'],\n",
    "            'feature': res['config']['feature'],\n",
    "            'accuracy': res['metrics'].get('accuracy'),\n",
    "            'roc_auc_ovr': res['metrics'].get('roc_auc_ovr')\n",
    "        }\n",
    "        all_results.append(flat_res)\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "print(f\"Loaded {len(df_results)} experiment results.\")\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance Summary Table\n",
    "\n",
    "This table summarizes the main findings, showing the performance of each model and feature combination. The results are sorted by accuracy to highlight the best-performing approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table for a clear overview\n",
    "performance_summary = df_results.pivot_table(\n",
    "    index='model',\n",
    "    columns='feature',\n",
    "    values='accuracy'\n",
    ").sort_values(by='combined', ascending=False) # Sort by our best feature set\n",
    "\n",
    "# Display formatted table\n",
    "performance_summary.style.format(\"{:.4f}\").background_gradient(cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visual Comparison of Model Accuracy\n",
    "\n",
    "A bar chart provides an intuitive visual comparison of the key results, making it easy to see the superiority of the MLP model with combined features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "ax = sns.barplot(data=df_results, x='model', y='accuracy', hue='feature', palette='viridis')\n",
    "\n",
    "ax.set_title('Model Accuracy Comparison by Feature Set', fontsize=16)\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax.set_ylim(0, 1.0)\n",
    "plt.legend(title='Feature Set')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. In-depth Analysis (Placeholder)\n",
    "\n",
    "The cells below serve as templates for more detailed analyses, such as plotting ROC curves or feature importances. To fully execute them, `run_benchmark.py` would need to be modified to save not just the metrics, but also the raw predictions, probabilities, and/or trained model objects.\n",
    "\n",
    "### 4.1. ROC Curve Analysis\n",
    "This requires saving `y_test` and `y_prob` for each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder: This code demonstrates the logic for plotting ROC curves.\n",
    "# To run, you would first need to load the saved true labels and prediction probabilities.\n",
    "\n",
    "def plot_roc_placeholder():\n",
    "    # --- This is mock data for demonstration --- #\n",
    "    # In a real run, load this from files, e.g.:\n",
    "    # y_test = np.load('results/y_test.npy')\n",
    "    # y_prob_mlp = np.load('results/mlp_combined_probs.npy')\n",
    "    # y_prob_cnn = np.load('results/cnn_raw_probs.npy')\n",
    "    num_classes = 30\n",
    "    y_test = np.random.randint(0, num_classes, 1000)\n",
    "    y_prob_mlp = np.random.rand(1000, num_classes)\n",
    "    y_prob_cnn = np.random.rand(1000, num_classes)\n",
    "    # Normalize probabilities to sum to 1\n",
    "    y_prob_mlp /= y_prob_mlp.sum(axis=1, keepdims=True)\n",
    "    y_prob_cnn /= y_prob_cnn.sum(axis=1, keepdims=True)\n",
    "    # --- End of mock data ---\n",
    "    \n",
    "    # Binarize the output for multi-class ROC\n",
    "    y_test_bin = pd.get_dummies(y_test).values\n",
    "\n",
    "    # Compute ROC for MLP\n",
    "    fpr_mlp, tpr_mlp, _ = roc_curve(y_test_bin.ravel(), y_prob_mlp.ravel())\n",
    "    roc_auc_mlp = auc(fpr_mlp, tpr_mlp)\n",
    "\n",
    "    # Compute ROC for CNN\n",
    "    fpr_cnn, tpr_cnn, _ = roc_curve(y_test_bin.ravel(), y_prob_cnn.ravel())\n",
    "    roc_auc_cnn = auc(fpr_cnn, tpr_cnn)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr_mlp, tpr_mlp, color='darkorange', lw=2, label=f'MLP (Combined) - micro-avg AUC = {roc_auc_mlp:.2f}')\n",
    "    plt.plot(fpr_cnn, tpr_cnn, color='navy', lw=2, label=f'CNN (Raw) - micro-avg AUC = {roc_auc_cnn:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Micro-Average ROC Curve Comparison')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_placeholder()"
   ]
  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Feature Importance\n",
    "This requires saving the trained model object, for instance, a Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder: This demonstrates how to plot feature importances from a trained RF model.\n",
    "# To run, you would first need to load a saved model, e.g., from a pickle file.\n",
    "\n",
    "def plot_feature_importance_placeholder():\n",
    "    # --- This is mock data for demonstration --- #\n",
    "    # In a real run, load a trained RF model and feature names\n",
    "    # from sklearn.ensemble import RandomForestClassifier\n",
    "    # model = load('results/rf_combined_model.pkl') \n",
    "    # feature_names = get_feature_names() # A helper function to get feature names\n",
    "    feature_names = [f'freq_{i}' for i in range(256)] + [f'dwt_{i}' for i in range(12)]\n",
    "    importances = np.random.rand(len(feature_names))\n",
    "    # --- End of mock data ---\n",
    "\n",
    "    # Create a pandas series for easy sorting and plotting\n",
    "    feat_importances = pd.Series(importances, index=feature_names)\n",
    "    top_20 = feat_importances.nlargest(20)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x=top_20.values, y=top_20.index, palette='mako')\n",
    "    plt.title('Top 20 Feature Importances (Placeholder)')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_importance_placeholder()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
